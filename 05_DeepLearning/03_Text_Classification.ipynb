{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01_Text Classification",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introducción\n",
        "\n",
        "Este ejemplo muestra cómo hacer una clasificación de texto a partir de texto sin formato (como un conjunto de archivos de texto en el disco). Demostramos el flujo de trabajo en el conjunto de datos de clasificación de sentimientos de IMDB (versión sin procesar). Usamos la capa **TextVectorization** para dividir e indexar palabras.\n",
        "\n",
        "https://www.imdb.com/"
      ],
      "metadata": {
        "id": "i-WxZtiFLVKH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ajq7l7zMH5eB"
      },
      "outputs": [],
      "source": [
        "# importamos las librerías\n",
        "import tensorflow as tf\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6NPgi2yLRvO",
        "outputId": "de1d17d3-524e-4e08-a90d-391430151018"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  27.9M      0  0:00:02  0:00:02 --:--:-- 27.8M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Buscamos un ejemplo\n",
        "!cat /content/aclImdb/train/pos/6248_7.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWSybK_DL40C",
        "outputId": "7ab5e624-695b-4688-9ce7-f3d05e374786"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Being an Austrian myself this has been a straight knock in my face. Fortunately I don't live nowhere near the place where this movie takes place but unfortunately it portrays everything that the rest of Austria hates about Viennese people (or people close to that region). And it is very easy to read that this is exactly the directors intention: to let your head sink into your hands and say \"Oh my god, how can THAT be possible!\". No, not with me, the (in my opinion) totally exaggerated uncensored swinger club scene is not necessary, I watch porn, sure, but in this context I was rather disgusted than put in the right context.<br /><br />This movie tells a story about how misled people who suffer from lack of education or bad company try to survive and live in a world of redundancy and boring horizons. A girl who is treated like a whore by her super-jealous boyfriend (and still keeps coming back), a female teacher who discovers her masochism by putting the life of her super-cruel \"lover\" on the line, an old couple who has an almost mathematical daily cycle (she is the \"official replacement\" of his ex wife), a couple that has just divorced and has the ex husband suffer under the acts of his former wife obviously having a relationship with her masseuse and finally a crazy hitchhiker who asks her drivers the most unusual questions and stretches their nerves by just being super-annoying.<br /><br />After having seen it you feel almost nothing. You're not even shocked, sad, depressed or feel like doing anything... Maybe that's why I gave it 7 points, it made me react in a way I never reacted before. If that's good or bad is up to you!"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Realizamos el paso de preprocesado\n",
        "\n",
        "`tf.keras.preprocessing.text_dataset_from_directory`"
      ],
      "metadata": {
        "id": "zXjNuyQ9MNHU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creamos el flujo de train\n",
        "batch_size = 32\n",
        "raw_train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    \"/content/aclImdb/train\",\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset=\"training\",\n",
        "    seed=2021,\n",
        ")\n",
        "raw_val_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    \"/content/aclImdb/train\",\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset=\"validation\",\n",
        "    seed=2021,\n",
        ")\n",
        "raw_test_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    \"/content/aclImdb/test\",\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "print(f\"Número de batch en raw_train es {raw_train_ds.cardinality()}\")\n",
        "print(f\"Número de batch en raw_validation es {raw_val_ds.cardinality()}\")\n",
        "print(f\"Número de batch en raw_test es {raw_test_ds.cardinality()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gBgdSs-eMXfa",
        "outputId": "bcea0dcf-5ded-47bf-ecc9-8ff33f06a040"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 75000 files belonging to 3 classes.\n",
            "Using 60000 files for training.\n",
            "Found 75000 files belonging to 3 classes.\n",
            "Using 15000 files for validation.\n",
            "Found 25000 files belonging to 2 classes.\n",
            "Número de batch en raw_train es 1875\n",
            "Número de batch en raw_validation es 469\n",
            "Número de batch en raw_test< es 782\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Observamos con un ejemplo realizando un batch\n",
        "for text_batch, label_batch in raw_train_ds.take(1):\n",
        "  for i in range(5):\n",
        "    print(text_batch.numpy()[i])\n",
        "    print(label_batch.numpy()[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cs97q66fNxGh",
        "outputId": "4afb691b-a140-4a3b-e589-648365a26a09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'Out of the first five episodes of Hammer\\'s short-running \"Hammer House of Horror\" series, this fifth episode with the wonderful title \"The House that Bled to Death\" is arguably the creepiest one. As a great fan of the Hammer Studios\\' Gothic Horror films for many years, I wonder what took me so long to finally start watching the series quite recently. So far, I\\'ve only seen the first five episodes, and I have a strong feeling that the best is yet to come, but even if the series stays as entertaining as the first five episodes are, I will be satisfied. Whereas the second and third episodes were great to watch for their morbid and ingeniously dark sense of humor, this fifth entry is definitely the one out of the first five that delivers the most genuine Horror. The episode begins when an elderly man murders his wife out of unknown motivations. Years later, William (Nicholas Ball) and Emma Peters (Rachel Davies) move in the house with their little daughter Sophie (Emma Ridley). Soon after moving in, however, the family have to find out that there is something terribly wrong with the house, which is seemingly haunted... The second episode directed by Francis Megahy is a lot better than his mediocre previous entry, \"Growing Pains\" (Episode 4), and the fairly unknown actors deliver good performances. The film is also well-made in terms of effects, cinematography and score. \"The House that Bled to Death\" is a solid episode that delivers the elements that my fellow Hammer-fans should like to see in a Short Horror tale. The film delivers a creepy atmosphere, genuine scare moments and intelligent twists, and is suspenseful and highly entertaining from the beginning to the end. Overall, this is highly recommendable to Hammer fans.'\n",
            "1\n",
            "b\"When a film boasts a cast list comprised of some of the most talented Hollywood actors in Bill Murray, Tim Robbins, Martin Landau, and Toby Jones, not to mention newcomer and extraordinary talent Saoirse Ronan, it will most likely go one of two ways: Either the film deserves such a fine cast and will be a great film (take 'The Departed' (2006) for example) . . . or the cast is set in a subpar piece to make up for the film's lesser elements (like 'Valentine's Day' (2010)). Luckily, 'City of Ember' falls in closer with the former instead of the latter.<br /><br />The film, adapted from the novel by Jeanne Duprau, tells the story of a small city named Ember that, over two hundred years prior, was locked away by the city's founding builders. The city is run solely on the power of its massive generator that lies beneath like a beating heart. However, the old generator is slowly falling apart, casting the city into short periods of darkness. Doon Harrow (Harry Treadway), a young boy recently assigned to work on the pipes of the city, is convinced that only he can fix the generator and help the city. During his explorations of the piping systems, he finds a mysterious room that is unmarked on his maps. With the help of Lina Mayfleet (Saoirse Ronan), the two teens discover the secrets of Ember and the answers that may help save the townspeople from eternal darkness.<br /><br />When 'City of Ember' was released into theatres, it seemed to have gone by the attention of most people without much notice. With a production budget of over $50 million, the film only raked in about $8 million domestically. But, why was this film such a tremendous bomb? From the looks of it, it had little to do with the film's quality. It has a very solid cast behind it, the story is entertaining, kids movies at this level typically do well, and the trailers made it look to be very beautiful (which it was). It would seem, then, that the film was simply just undermarketed. Very little attention was given by the marketing team for this one and that's what killed it.<br /><br />Had 'City of Ember' been given a proper marketing campaign, it could've been very successful. The film is really quite good. The acting, as expected, is fantastic. The cast of legends like Bill Murray & Martin Landau were fantastically chosen; and, the 'Lovely Bones' star Saoirse Ronan is quickly becoming one of the best actors of her age group, not to mention a definite for a few Oscars in her lifetime. The story is very entertaining, even if it isn't the most original or mysterious. The set design, however, is really what sets the film apart. Ember is built into a fantastic and beautiful city, from the underground and above. One issue that many seem to have deals with the CGI effects in the film, especially some of the scenes towards the end. While some of the CGI is spotty, it's hardly distracting enough to remove from the overall beauty of the film and no one should allow such small issues to detract wholly from an overall good film.<br /><br />While book purists appear not to love the film for the oversimplification of the adaptation, those who have not read Duprau's novel should have a good time with it.<br /><br />Final Verdict: 8/10.<br /><br />-AP3-\"\n",
            "2\n",
            "b'At date of writing this film has yet to earn 350 votes in the IMDb, meaning it falls short of the top 250 despite it\\'s 7+ score. If you\\'re somehow reading this without having seen the film, then do yourself a large favour: rent it, or buy it.<br /><br />It\\'s ironic that the production company of the \"quiet\", peaceful Beatle, George Harrison, should construct such a violent and gritty picture. Bob Hoskins is so perfect as an East-End ganglord that you wonder how he ever got cast out of character as a cod American acting opposite a cartoon rabbit. Is this really the same man who urged the virtues of conversation on behalf of a phone company? Hoskins acts more in the final two minutes of this movie than in the rest of his career put together.<br /><br />What makes The Long Good Friday so special, (apart from stylish, but not ostentatious, direction and a superb theme score) is its variation on the typical crime theme. Instead of guns and bluster stretched out over an hour and forty minutes we get what is essentially a detective story. Who is planting bombs on Harold\\'s patch? Who is trying to undermine his organisation? And will Harold put the pieces together before the noose around his neck gets too tight? Admittedly, Sherlock Holmes never got his clues by stringing up Moriarty from his ankles in a meat factory, but the narrative principle is the same.<br /><br />Is it really already the 20th anniversary of this film? What a timely reminder of how powerful the British Film Industry used to be. A must-see movie and quite excellent.'\n",
            "2\n",
            "b'Good sequel to Murder in a Small Town. In this one Cash and his police Lt. buddy unravel a sticky plot involving a Nazi criminal, a philanthropic witch, and a family of screw-ups and their wierdo helpers. As in the original, the viewer is treated to a nice little mystery with distinctive sights and sounds of pre-war America. Go see it.'\n",
            "1\n",
            "b\"A very good action film. What made this one original was the difference in fighting style between the 2 leads first lead: roddy piper, who does the wrestling style. Second lead, blank who does the martial arts style<br /><br />It makes the fight scenes so cool, and original, especially the fight between piper and blank. The story, from what I can remember is good enough. It's not a brilliant story. But the story doesn't make or break the film. It's the real excellent fights that make this film. Obviously, this is not a film that girls would like. Girl tend not to appreciate a witty fight scene, girls would prefer it if the wit was in the story, which they won't get in this film. Hence this film wasn't rated higher (females gave it low ratings - see the statistics) But I am a man, so I loved this film :)<br /><br />\"\n",
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Parte de preprocesado y limpieza <br />\n",
        "import re\n",
        "import string\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "def custom_standarization(input_data):\n",
        "  lowercase = tf.strings.lower(input_data)\n",
        "  stripped_html = tf.strings.regex_replace(lowercase, \"(<br />){1,}\", \" \")\n",
        "  return tf.strings.regex_replace(\n",
        "      stripped_html, f\"[{re.escape(string.punctuation)}]\", \"\"\n",
        "  )"
      ],
      "metadata": {
        "id": "82P81gQlOVZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Realizamos el paso de vectorización\n",
        "# definimos unas constantes\n",
        "max_features = 20000\n",
        "embedding_dim = 128\n",
        "sequence_length = 500\n",
        "\n",
        "# Creamos nuestra capa de vectorización\n",
        "vectorize_layer = TextVectorization(\n",
        "    standardize=custom_standarization,\n",
        "    max_tokens=max_features,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length,\n",
        ")\n",
        "\n",
        "# Aplicamos al solo texto\n",
        "text_ds = raw_train_ds.map(lambda x, y: x)\n",
        "vectorize_layer.adapt(text_ds)"
      ],
      "metadata": {
        "id": "fsFMnAtNPrVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorize_layer.output_shape()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "TFZDY4hOVPks",
        "outputId": "023a517c-fe51-4387-b0c3-cc2faf5333f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-5c59fc9a0acc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvectorize_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36moutput_shape\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2226\u001b[0m     \"\"\"\n\u001b[1;32m   2227\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inbound_nodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2228\u001b[0;31m       raise AttributeError(f'The layer \"{self.name}\" has never been called '\n\u001b[0m\u001b[1;32m   2229\u001b[0m                            'and thus has no defined output shape.')\n\u001b[1;32m   2230\u001b[0m     all_output_shapes = set(\n",
            "\u001b[0;31mAttributeError\u001b[0m: The layer \"text_vectorization_3\" has never been called and thus has no defined output shape."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplicamos la vectorización a nustros datos\n",
        "# opción 1\n",
        "\n",
        "# text_input = tf.keras.Input(shape=(1,), dtype=tf.string, name='text')\n",
        "# x = vectorize_layer(text_input)\n",
        "# y = layers.Embedding(max_features + 1, embedding_dim)(x)"
      ],
      "metadata": {
        "id": "vuP0CPG_RrpD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Opción 2 en GPU / CPU\n",
        "def vectorize_text(text, label):\n",
        "    text = tf.expand_dims(text, -1)\n",
        "    return vectorize_layer(text), label\n",
        "\n",
        "\n",
        "# Vectorize the data.\n",
        "train_ds = raw_train_ds.map(vectorize_text)\n",
        "val_ds = raw_val_ds.map(vectorize_text)\n",
        "test_ds = raw_test_ds.map(vectorize_text)\n",
        "\n",
        "# Do async prefetching / buffering of the data for best performance on GPU.\n",
        "train_ds = train_ds.cache().prefetch(buffer_size=10)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=10)\n",
        "test_ds = test_ds.cache().prefetch(buffer_size=10)"
      ],
      "metadata": {
        "id": "AiB5zutLSZW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Construimos el modelo\n",
        "\n",
        "Creamos un modelo simple de 1D convnet con una capa (layer Embedding)."
      ],
      "metadata": {
        "id": "_Db0alxxSl98"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "# A integer input for vocab indices.\n",
        "inputs = tf.keras.Input(shape=(None,), dtype=\"int64\")\n",
        "\n",
        "# Next, we add a layer to map those vocab indices into a space of dimensionality\n",
        "# 'embedding_dim'.\n",
        "x = layers.Embedding(max_features, embedding_dim)(inputs)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "\n",
        "# Conv1D + global max pooling\n",
        "x = layers.Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3)(x)\n",
        "x = layers.Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3)(x)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "\n",
        "# We add a vanilla hidden layer:\n",
        "x = layers.Dense(128, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "\n",
        "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
        "predictions = layers.Dense(1, activation=\"sigmoid\", name=\"predictions\")(x)\n",
        "\n",
        "model = tf.keras.Model(inputs, predictions)\n",
        "\n",
        "# Compile the model with binary crossentropy loss and an adam optimizer.\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "juu6KsjpSu-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenamos el modelo con epoch = 1\n",
        "epoch = 1\n",
        "\n",
        "# Ajustamos el modelo a train y test dataset\n",
        "model.fit(train_ds, validation_data=val_ds, epochs=epoch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qdBg4VduTAnl",
        "outputId": "383a5ddf-06e1-4bae-a276-5beef745c646"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1875/1875 [==============================] - 49s 26ms/step - loss: -334789541888.0000 - accuracy: 0.1661 - val_loss: -1830001180672.0000 - val_accuracy: 0.1685\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f042db15990>"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluando el modelo con los datos de test\n",
        "model.evaluate(test_ds )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Twf8YEpyTTz8",
        "outputId": "a86b0981-3e62-4965-c37b-41f5004fe36e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 11s 14ms/step - loss: 1846528311296.0000 - accuracy: 0.5000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1846528311296.0, 0.5]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creación del modelo final\n",
        "# A string input\n",
        "inputs = tf.keras.Input(shape=(1,), dtype=\"string\")\n",
        "# Turn strings into vocab indices\n",
        "indices = vectorize_layer(inputs)\n",
        "# Turn vocab indices into predictions\n",
        "outputs = model(indices)\n",
        "\n",
        "# Our end to end model\n",
        "end_to_end_model = tf.keras.Model(inputs, outputs)\n",
        "end_to_end_model.compile(\n",
        "    loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "# Test it with `raw_test_ds`, which yields raw strings\n",
        "end_to_end_model.evaluate(raw_test_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1G5OB7_VBDX",
        "outputId": "06a5a3cf-605c-425e-b3dd-5d30b321220a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 12s 15ms/step - loss: 1846527787008.0000 - accuracy: 0.5000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1846527787008.0, 0.5]"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    }
  ]
}